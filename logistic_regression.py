# -*- coding: utf-8 -*-
"""Logistic_Regression.ipynb

Automatically generated by Colaboratory.

"""

import pandas as pd
import numpy as np
import csv
import random
import networkx as nx
from sklearn.linear_model import LogisticRegression
from operator import itemgetter

"""Section 1: Using Node IDs, Degree of each node pair and common neighbors between node pairs as features"""

#Code to Create Adjacency Matrix from training data.

all_nodes = []
matrix = np.zeros((4085,4085), dtype = int)
train = open("train.txt", "r")
for line in train:
  lis = line.split()
  all_nodes.append(int(lis[0]))
  for i in range(1, len(lis)):
    matrix[int(lis[0])][int(lis[i])] = 1
    matrix[int(lis[i])][int(lis[0])] = 1

np.savetxt('output.csv', matrix, delimiter=",")
all_nodes = list(set(all_nodes))
train.close()

print(len(all_nodes))
print(len(matrix))

#Read from training data as an adjacency list and add missing nodes manually
g = nx.read_adjlist("train.txt")
g.add_nodes_from([32, 1611, 2008, 2451, 2692, 2837])

#Extract all Negative Samples
negs = []
nodes = g.nodes
for i in range(len(matrix)):
  for j in range(i, len(matrix[i])):
    if i!= j and matrix[i][j] == 0 and str(i) in nodes and str(j) in nodes:
        negs.append([str(i), str(j), g.degree[str(i)], g.degree[str(j)], len(sorted(nx.common_neighbors(g, str(i), str(j))))])
negs = sorted(negs, key = itemgetter(4))

print(len(negs))

#Shuffle all negative samples
shuffled = random.sample(negs, len(negs))

#Choose around 20 times number of positive samples ~ 540K as negative samples after shuffling
negatives = pd.DataFrame.from_records(shuffled[:540000])

#Store all edge pairs from training data
edges = (g.edges)
print(len(edges))

#Extracting Positive samples into a dataframe

lst = [[]*26937]
j = 0
for i in edges:
  lst.append([])
  node1 = i[0]
  node2 = i[1]
  lst[j].extend([node1, node2, g.degree[node1], g.degree[node2], len(sorted(nx.common_neighbors(g, node1, node2)))])
  j+=1
positives = pd.DataFrame.from_records(lst)
positives.drop(26937,inplace = True)
positives

#Concatenate positive and negative samples into a single data frame
data = pd.concat([positives, negatives], ignore_index=True)
data

#Manually creating labels for positive and negative samples
pos = ['1']*26937
neg = ['0']*540000
total = pos + neg

labels = pd.DataFrame.from_records(total)
labels

#Fitting the data to the logistic regression model
lr = LogisticRegression(class_weight="balanced")
lr.fit(data, labels)

#Reading test data

with open('test-public.csv', newline='') as f:
    reader = csv.reader(f)
    test_data = list(reader)
test_data.pop(0)
print(test_data)

#Adding corresponding features for test data
for i in test_data:
  node1 = i[1]
  node2 = i[2]

  if g.has_node(node1):
    i.append(g.degree[node1])
  else:
      i.append(0)
  if g.has_node(node2):
    i.append(g.degree[node2])
  else:
    i.append(0)
  
  if g.has_node(node1) and g.has_node(node2):
    i.append(len(sorted(nx.common_neighbors(g, node1, node2))))
  else:
    i.append(0)
  
print(test_data)

#Putting Test data vectors into a data frame
test = pd.DataFrame.from_records(test_data)
test.drop(0, inplace = True, axis = 1)
test.reset_index
test

#Calculating predictions as probabilities on the test data using created model
predictions = lr.predict_proba(test)

#Extracting predictions of the 1's class
predict = predictions[:,1].tolist()

#Creating IDs for output file
id = []
for i in range(1,2001):
  id.append(i)

#Putting predictions into a dataframe
pred = pd.DataFrame()
pred["Id"] = id
pred["Predicted"] = predict
pred

#Writing predictions data frame to output file
pred.to_csv("preds.csv", index = False)

"""Section 2: Using features from nodes.json as added features to above model"""

#Reading from nodes.json
import json
with open('nodes.json') as f:
  features = json.load(f)

#Creating a revised dictionary with keys as the IDs
reformed = {}
for i in features:
  reformed[str(i['id'])] = i
print(reformed)

#Creating a list of vectors of 0-52 keywords for each author
keywords = []
j = 0
for i in reformed.keys():
  keywords.append([])
  for k in range(53):
    if (('keyword_' + str(k)) in reformed[i]):
      keywords[j].append(1)
    else:
      keywords[j].append(0)
  j+=1

#Creating a list of vectors of 0-347 venues for each author
venues = []
j = 0
for i in reformed.keys():
  venues.append([])
  for k in range(348):
    if (("venue_" + str(k)) in reformed[i]):
      venues[j].append(1)
    else:
      venues[j].append(0)
  j+=1

#Positive samples with common keywords and common venues as added features

lst = [[]*26937]
j = 0
for i in edges:
  Vcount = 0
  Kcount = 0
  lst.append([])
  node1 = i[0]
  node2 = i[1]
  lst[j].extend([node1, node2, g.degree[node1], g.degree[node2], len(sorted(nx.common_neighbors(g, node1, node2)))])
  
  for k in range(348):
    if venues[int(node1)][k] == 1 and venues[int(node2)][k] == 1:
      Vcount += 1
  
  for k in range(53):
    if keywords[int(node1)][k] == 1 and keywords[int(node2)][k] == 1:
      Kcount += 1

  lst[j].append(Kcount)
  lst[j].append(Vcount)
  
  j+=1
positives = pd.DataFrame.from_records(lst)
positives.drop(26937,inplace = True)

#Negative samples with Common Keywords and Common Venues as added features

negs = []
nodes = g.nodes
for i in range(len(matrix)):
  for j in range(i, len(matrix[i])):
    if i!= j and matrix[i][j] == 0 and str(i) in nodes and str(j) in nodes:
      Vcount = 0
      Kcount = 0
      
      for k in range(348):
        if venues[i][k] == 1 and venues[j][k] == 1:
          Vcount += 1

      for k in range(53):
        if keywords[i][k] == 1 and keywords[j][k] == 1:
          Kcount += 1
      

      negs.append([str(i), str(j), g.degree[str(i)], g.degree[str(j)], len(sorted(nx.common_neighbors(g, str(i), str(j)))), Kcount, Vcount])
negs = sorted(negs, key = itemgetter(4))

shuffled = random.sample(negs, len(negs))
print(len(shuffled))

negatives = pd.DataFrame.from_records(shuffled[:540000])

data = pd.concat([positives, negatives], ignore_index=True)
data

#Reading Test data
with open('test-public.csv', newline='') as f:
    reader = csv.reader(f)
    test_data = list(reader)
test_data.pop(0)
print(test_data)

#Adding corresponding features to test data
for i in test_data:
  node1 = i[1]
  node2 = i[2]
  Vcount = 0
  Kcount = 0
  if g.has_node(node1):
    i.append(g.degree[node1])
  else:
      i.append(0)
  if g.has_node(node2):
    i.append(g.degree[node2])
  else:
    i.append(0)
  if g.has_node(node1) and g.has_node(node2):
    i.append(len(sorted(nx.common_neighbors(g, node1, node2))))
  else:
    i.append(0)
  
  for k in range(53):
    if keywords[int(node1)][k] == 1 and keywords[int(node2)][k] == 1:
      Kcount += 1

  for k in range(348):
    if venues[int(node1)][k] == 1 and venues[int(node2)][k] == 1:
      Vcount += 1

  i.append(Kcount)
  i.append(Vcount)
  
print(test_data)

#Adding test data to a dataframe
test = pd.DataFrame.from_records(test_data)
test.drop(0, inplace = True, axis = 1)
test.reset_index

#Fitting the feature vectors to the logistic regression model and extracting predictions of 1's class
lr = LogisticRegression(class_weight="balanced", verbose = True)
lr.fit(data, labels)
predictions = lr.predict_proba(test)
predict = predictions[:,1].tolist()

#Creating output prediction file
pred = pd.DataFrame()
pred["Id"] = id
pred["Predicted"] = predict
pred
pred.to_csv("preds.csv", index = False)
